This section describes the expert controller and simulation framework used to generate training data, followed by the learning pipeline used to train a regression neural network controller via behavioural cloning.

\subsection{Expert PID Controller}

Each controlled variable $x_i$ is governed by a PID law of the form
\begin{equation}
u_i(t) = K_{p,i} e_i(t) + K_{i,i} \int_0^t e_i(\tau)\,d\tau + K_{d,i} \frac{d e_i(t)}{dt},
\end{equation}
where $e_i(t) = x_{i,\text{ref}}(t) - x_i(t)$ denotes the tracking error.

Independent PID controllers are implemented for the following objectives:
\begin{align*}
u_x &: \text{horizontal quadcopter position } (x_q), \\
u_z &: \text{vertical quadcopter position } (z_q), \\
u_\phi &: \text{payload swing damping } (\phi \rightarrow 0), \\
u_\theta &: \text{quadcopter pitch stabilisation } (\theta), \\
u_\ell &: \text{tether feed control } (\ell_{\text{ref}}(t)).
\end{align*}

A cascaded control structure is used, with outer-loop controllers regulating position and payload motion, and inner-loop controllers stabilising vehicle attitude and velocity. Additional details of the implementation are included in Appendix~\ref{ap:pid}. For simplicity, and to reduce the dimensionality of the learning problem, the neural network controller is trained assuming a fixed tether length; consequently, the tether feed controller is inactive during data generation.

The PID controller is not intended to be optimal, but rather to provide a reliable and interpretable expert suitable for supervised learning. As a result, the behaviour and safety properties of the learned neural network controller are inherently constrained by the coverage and quality of the expert demonstrations.

\subsection{Dataset Generation}

The expert PID-controlled simulation is used to generate training data for the NN controller. Trajectories are generated by sampling a range of initial conditions to reach a reference setpoint, and logging the resulting state, error, and control tuples at each simulation step. The data generation pipeline is outlined in Fig.~\ref{fig:data_gen}. At each time step $t$, the input to the learning model --the dataset features-- consists of the system state $\bm{s}_t$ together with a vector of tracking errors $\bm{e}_t$, defined with respect to the desired reference trajectory or setpoint:
\[
\bm{e}_t = \bm{s}_{\text{ref}}(t) - \bm{s}_t.
\]

The resulting dataset is given by
\[
\mathcal{D} = \{ (\bm{s}_t, \bm{e}_t), \bm{u}_t \}_{t=0}^{T},
\]
where $\bm{u} = \pi_{\text{PID}}(\bm{s})$ denotes the control action generated by the PID controller.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.82\linewidth]{figs/data-generation-pipeline.pdf}
    \caption{Dataset generation pipeline using expert PID controller in simulation. The numerical simulation framework used with the expert PID controller to update the system dynamics is detailed in Appendix~\ref{ap:num_sim_framework}.}
    \label{fig:data_gen}
\end{figure}



% This formulation defines a supervised regression problem in which the neural network is trained to approximate the expert policy mapping from states and tracking errors to continuous-valued control outputs.







% \subsection{Neural Network Controller}
% \subsubsection{Architecture}
% The controller is implemented as a feed-forward neural network that maps the system state $\bm{s}$ to continuous-valued control outputs $\bm{u}$. The network architecture is designed to support regression-based control and real-time inference, while remaining compatible with existing neural network verification tools.

% include number of layers, hidden dimensions, activation functions, and output normalisation etc

% \subsubsection{Training}

% Training is performed using supervised learning on the dataset generated by the expert controller. The neural network parameters are optimised to minimise a regression loss between the predicted control outputs and the expert commands.

% The training procedure, including dataset splitting, loss functions, optimisation algorithms, and hyperparameter choices, is described in detail in the following section.

% \subsubsection{Adversarial Training for Robustness}

% To improve robustness of the learned controller to bounded perturbations in the input state, adversarial training is applied during neural network training. Perturbations are introduced at the input level to simulate sensor noise and modelling uncertainty, encouraging the network to learn a smoother and more stable control policy.

\subsection{Neural Network Controller}

The learned controller is implemented as a feed-forward neural network that approximates the expert PID policy via supervised behavioural cloning. The network maps the current system state and tracking errors to continuous-valued control outputs, and is evaluated in closed loop using the nonlinear system dynamics described in Section~\ref{sec:modelling}.

\subsubsection{Architecture}

The controller is implemented as a fully connected multi-layer perceptron (MLP). The network takes as input the concatenation of the system state $\bm{s}_t$ and the corresponding tracking error vector $\bm{e}_t$, and outputs continuous-valued control commands:
\[
f_\theta : (\bm{s}_t, \bm{e}_t) \mapsto \bm{u}_t.
\]

The architecture consists of three 32--node hidden layers, each followed by a ReLU activation. Dropout can optionally be applied during training to improve generalisation. The output layer is linear, as the control task is formulated as a regression problem. 


\subsubsection{Training Procedure}

Training is done via supervised learning on the generated dataset. The network parameters $\theta$ are optimised to minimise a mean squared error (MSE) loss between the predicted control outputs and the expert data:
\[
\mathcal{L}_{\text{MSE}}(\theta) = \mathbb{E}_{(\bm{s}, \bm{e}, \bm{u}) \sim \mathcal{D}} \left[ \| f_\theta(\bm{s}, \bm{e}) - \bm{u} \|_2^2 \right].
\]

The dataset is split into training, validation, and test subsets (80/10/10\% respectively). We train for 100 epochs using a standard gradient-based optimiser, with early stopping based on validation loss to mitigate overfitting. Model performance is evaluated using both mean squared error (MSE) and mean absolute error (MAE) on held-out test data.

\subsubsection{Closed-Loop Deployment}

After training, the neural network controller replaces the expert PID controller in the closed-loop simulation. At each control step, the current system state and tracking error are passed to the network to generate control commands, which are then applied to the system dynamics.

Although the network is trained to imitate a stabilising expert policy, no formal guarantees of stability or safety are provided by the learning process alone. In particular, the learned controller may exhibit unstable behaviour for states outside the training distribution or under perturbations, motivating the robustness analysis and formal verification presented in later sections.

% \subsubsection{Adversarial and Robustness Training}

% To improve robustness of the learned controller to bounded perturbations in the input state, adversarial training is incorporated into the learning process. During training, adversarial perturbations are applied to the input features to simulate sensor noise and modelling uncertainty. The network is trained to minimise the worst-case loss within a bounded perturbation set, encouraging smoother input--output behaviour and reducing sensitivity to small disturbances. In addition, to further enhance

% The impact of adversarial training is evaluated through robustness metrics, explainability analyses, and formal verification results, which are discussed in subsequent sections.

\subsubsection{Adversarial and Property-Driven Training (PDT)}

% MAKE THIS WAY SHORTER

% 4.4 make a section on adv

Beyond standard supervised training, additional training objectives are introduced to improve robustness and enforce safety-relevant properties in the learned controller. Two complementary strategies are considered: adversarial training and property-driven training. The integration of these strategies into the full closed-loop NN controller system is shown in Fig.~\ref{fig:training_pipeline}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/training_pipeline.pdf}
    \caption{Neural network controller pipeline with adversarial and property-driven training. Adversarial training and PDT are applied during model training. During inference, the trained network is deployed in closed loop with the system dynamics, and adversarial examples are generated by perturbing the input state within a bounded set. This allows for controllers, trained with or without adversarial objectives or PDT, to be evaluated for robustness to noise.}
    \label{fig:training_pipeline}
\end{figure}

The effectiveness of these strategies is evaluated through different verification techniques in Section~\ref{sec:verification}. 
% The adversarial training implementation and adversarial example generation methods are described in detail in the next section (Section~\ref{sec:adv_training_implementation}), and the PDT implementation of formal properties into the training loss is described in Section~\ref{sec:controller_verification}.


\subsection{Adversarial Training and Adversarial Example Generation}
\label{sec:adv_training_implementation}

This section describes the adversarial training procedure used to improve the robustness of the learned controller, as well as the method for generating adversarial examples during verification and evaluation.

\subsubsection{Threat model and attack objective}
Let the controller be a regression model $f_{\theta}:\mathbb{R}^{d}\rightarrow \mathbb{R}^{k}$ mapping an input feature/state vector $x$ to a control output $\hat{y}=f_{\theta}(x)$. We consider worst-case bounded perturbations $\delta$ applied to the input,
\begin{equation}
x_{\mathrm{adv}} = x + \delta,
\qquad
\|\delta\|_{\infty}\le \varepsilon \ \ \text{or}\ \ \|\delta\|_2\le \varepsilon,
\end{equation}
and generate adversarial examples by maximising the MSE loss:
\begin{equation}
\max_{\|\delta\|\le \varepsilon}\ \mathcal{L}\big(f_\theta(x+\delta),y\big),
\qquad
\mathcal{L}(\hat{y},y)=\|\hat{y}-y\|_2^2.
\end{equation}
All reported attacks are \emph{white-box}: they use gradients of $\mathcal{L}$ with respect to the perturbation (PGD) under the current model parameters.

\subsubsection{Attacks considered}
We evaluate two standard white-box attacks adapted to regression, using MSE as the attack loss
$\mathcal{L}(f_\theta(x),y)=\|f_\theta(x)-y\|_2^2$, where $x$ is the clean input, $y$ the target, and
$\nabla_x$ denotes the gradient with respect to the input.

\paragraph{FGSM (single-step).}
FGSM takes one gradient-ascent step to increase $\mathcal{L}$.
For an $\ell_\infty$ constraint,
\begin{equation}
x_{\mathrm{adv}} = x + \varepsilon \cdot \mathrm{sign}\!\left(\nabla_x \mathcal{L}\big(f_\theta(x),y\big)\right),
\end{equation}
where $\varepsilon$ is the perturbation budget and $\mathrm{sign}(\cdot)$ applies the gradient direction per feature.
For $\ell_2$, FGSM uses the normalised gradient direction.

\paragraph{PGD (iterative).}
PGD iteratively updates a perturbation $\delta_t$ for $T$ steps and projects back to the $\varepsilon$-ball:
\begin{align}
\delta_{t+1} &= \Pi_{\|\delta\|\le \varepsilon}\Big(\delta_t + \alpha \cdot g_t\Big),
\qquad
x_{\mathrm{adv}} = x + \delta_T,
\end{align}
where $\alpha$ is the step size and $\Pi_{\|\delta\|\le \varepsilon}$ enforces the norm constraint (clipping for $\ell_\infty$, rescaling for $\ell_2$).
The ascent direction is
\begin{align}
g_t &=
\begin{cases}
\mathrm{sign}\!\left(\nabla_{\delta}\mathcal{L}\big(f_\theta(x+\delta_t),y\big)\right), & \ell_\infty, \\
\frac{\nabla_{\delta}\mathcal{L}\big(f_\theta(x+\delta_t),y\big)}{\left\|\nabla_{\delta}\mathcal{L}\big(f_\theta(x+\delta_t),y\big)\right\|_2}, & \ell_2,
\end{cases}
\end{align}
i.e., sign-gradient steps under $\ell_\infty$ and unit-gradient steps under $\ell_2$.

We use \emph{random start} by sampling $\delta_0$ within the $\varepsilon$-ball to strengthen the attack.

\subsubsection{Implementation details and evaluation protocol}
Adversarial inputs are generated via a unified attack factory (\texttt{make\_attack\_fn}) and applied \emph{per mini-batch} during training/evaluation. Concretely, for each batch $(X,y)$ we compute $X_{\mathrm{adv}}=\texttt{attack\_fn}(f_\theta,X,y)$ and then evaluate the model on $X_{\mathrm{adv}}$ to obtain adversarial MSE/MAE. Clean evaluation is performed by setting \texttt{attack\_fn=None}.

For qualitative inspection, we also generate a single adversarial sample from the test set and plot the clean vs.\ adversarial feature vector (Figure~\ref{fig:feature_perturbations}). This highlights how PGD produces structured, loss-increasing perturbations across features rather than unstructured random noise.

\paragraph{Adversarial training parameters.}
Our adversarial training configuration uses \textbf{PGD} with $\ell_\infty$ perturbations, perturbation budget $\varepsilon=0.05$, and $T=40$ update steps (with default $\alpha=\varepsilon/T$). We use \texttt{mode=mix} with $\lambda=0.5$ (i.e., every sample has a 0.5 probability of being attacked when adversarial training is enabled).

When enabled, adversarial training forces the controller to fit not only the nominal data distribution, but also the worst-case perturbations within the chosen threat model. Empirically, this typically reduces the growth of adversarial error as $\varepsilon$ increases (at the cost of additional computation, since generating $X_{\mathrm{adv}}$ requires multiple forward/backward passes per batch). The robustness curves in Figure~\ref{fig:robustness_curve_pgd} illustrate this effect by comparing a normally trained model and an adversarially trained model under PGD evaluation.

